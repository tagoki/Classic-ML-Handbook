# Основы градиентного спуска

## Метод градиентного спуска  
**Метод градиентного спуска** — это фундаментальный метод обучения параметров моделей, основанный на градиенте функции по её параметрам. Далее рассмотрим основные детали метода.

## Про математический аспект  
Не буду сильно углубляться в математический фундамент, пройдемся по основной идее, которая даст интуитивное понимание и дальнейший импульс для изучения данной темы. Мы начнем с самых основ и дойдем до реализации данного метода на Python.

## Что такое производная  
Производная — это новая функция, показывающая, как изменяется и в каком направлении возрастает или убывает наша исходная функция, от которой мы брали производную.

Это мощный инструмент, помогающий исследовать функции, но нас интересует один из её аспектов — нахождение минимума.

## Применение производной  
Перед тем как переходить к многомерным пространствам, рассмотрим простой пример производной от функции одной переменной:

$$
f(x) = x^2
$$

Это обычная квадратичная функция, имеющая график параболы. Главная задача производной в контексте машинного обучения — найти минимум функции, то есть найти такие значения x, при которых функция f(x) достигает минимального значения. Не сложно догадаться, что функция принимает минимальное значение при x = 0 при таком x значение функции f тоже равно 0.

Хорошо, это понятно, а как найти производную функции:

$$
f(x) = x^2 + 4x + 5
$$

Рассмотрим аналитическое решение данной задачи:  
1) Найдём производную функции.  
2) Приравняем эту производную к нулю и найдём минимум функции.

![[images/Pasted image 20250722133703.png]]

Вдаваться в детали решения не обязательно, но если интересно, первое действие — это применение правил из таблиц по дифференцированию, подробнее можно узнать в интернете или [в этой статье](https://habr.com/ru/articles/897710/).

Рассмотрим график последней функции.

![[images/Pasted image 20250722134107.png]]

Действительно, функция принимает минимальное значение при x = -2.

## Алгоритмическое решение  

```python
import numpy as np  # импорт библиотеки для работы с массивами

np.random.seed(12)  # фиксируем псевдослучайные числа

x = np.random.randint(-100, 100)  # выбираем случайное число

def f(x):  # исходная функция (не используется в коде, но важно знать, откуда берём производную)
    return x**2 + 4*x + 5

def diff_f(x):  # производная функции f(x)
    return 2*x + 4

def grad(x):
    learning_rate = 0.01  # скорость обучения. Чем больше — тем быстрее, но точность уменьшается
    iterations = 10000  # количество итераций

    for _ in range(iterations):
        value = diff_f(x)  # вычисление производной в текущей точке
        x = x - learning_rate * value  # обновление значения x

    return x

print(grad(x))

```
## Идея градиентного спуска  
Всё просто и понятно: как и в аналитическом решении, мы должны найти минимум функции, но оно работает не всегда. В многомерных функциях (об этом дальше) эффективно помогает только градиентный спуск. В чём разница?  

Главная разница в том, что мы сначала берём случайную точку и вычисляем там значение производной. Далее обновляем значение по формуле:  
$$
x_{\text{новый}} = x_{\text{старый}} - \eta \cdot f'(x_{\text{старый}})
$$
где:  
- x_новый— новое значение \(x\),  
- x_старый — старое значение \(x\),  
- eta — скорость обучения,  
- f'x_старый — значение производной в старой точке.

Неформально говоря, мы спускаемся с случайной точки к минимуму, уменьшая \(x\).

![[images/Pasted image 20250722153550.png]]

Метод градиентного спуска — это итерационный метод, так как он повторяется несколько раз.

Мы рассмотрели случай градиентного спуска в двумерном пространстве, применив его к функции одной переменной. Далее рассмотрим градиентный спуск в трёхмерном и \(n\)-мерном пространстве, а также с помощью градиентного спуска подберём параметры для линейной регрессии. Этот раздел можно считать введением во что-то большее.


# Практическое применение градиентного спуска на примере линейной регрессии 

## Оптимизация линейной регрессии при помощи градиентного спуска  
Хотя для линейной регрессии существует [[0_1_2 Аналитическое решение линейной регрессии|аналетическое решение]], оно имеет ряд недостатков и ограничений, например, большие затраты на вычисления. Это делает градиентный спуск более привлекательным в задачах оптимизации линейной регрессии.

Однако есть различия: линейная регрессия — это функция от множества переменных, и её оптимизация отличается от оптимизации обычной функции. Поэтому снова пройдемся по основам.
## Основы градиента  

### Что такое частная производная  
Частная производная — это метод взятия производной (дифференцирования) от функции нескольких переменных, при котором одна из величин берется как константа, а другая дифференцируется по обычным правилам.

Возьмём производную от функции:  
$$
f(x,y) = x^2 + 2y^2
$$

Записи  
$$
\frac{df}{dx}
$$  
и  
$$
\frac{df}{dy}
$$  
означают, что мы берём производную функции \(f\) по аргументу \(x\) или \(y\). Вдаваться в подробности не обязательно, но рекомендуется для полного понимания.

Что такое частная производная? Поскольку мы работаем с функциями нескольких переменных, мы оказываемся в многомерном пространстве, где график — поверхность.

В функции одной переменной мы берём производную только по этой переменной и смотрим изменения функции по ней, так как переменная одна. В функциях нескольких переменных переменных несколько, и возникает вопрос: как изучать изменения функции, если переменных несколько?

Математики придумали частную производную, которая работает так же, как обычная производная, но с учётом фиксирования других переменных:

- Берём первую переменную,  
- Берём вторую переменную,  
- При вычислении производной по одной переменной другую считаем константой,  
- Дифференцируем по выбранной переменной как обычно.

Сначала смотрим изменения функции при изменении \(x\), не изменяя \(y\), затем — при изменении \(y\), не изменяя \(x\).

### Что такое градиент  
Градиент — это вектор, состоящий из частных производных функции по её переменным, показывающий направление наибольшего роста функции.  
![[images/______________.webp]]  
То есть, если не вдаваться в вычисления (ниже), то градиент — это стрелка, которая говорит (более формально — указывает), куда надо изменять параметры, чтобы достичь максимума функции.  

![[images/Pasted image 20250723174517.png]]  
$$
\nabla f = (\cos(x), -\sin(y))
$$  
Это градиент, то есть вектор.  

Важно отметить, что вектор один. Поле векторов (как на рисунке, много векторов) образуется тогда, когда мы рассматриваем градиент в разных точках. Чем больше точек — тем больше векторов.

## Правило дифференцирования сложной функции  
Честно, это последнее, что ждёт вас из математики.  

### Что такое сложная функция  
Сложная функция — это функция, имеющая вид:  
$$
f(u)
$$  
где:  
$$
u = g(x)
$$  
То есть:  
$$
f(g(x))
$$  
==Такая функция f, которая принимает в себя результат функции g, называется сложной функцией.==  

Более формально:  
$$
f(x) = h(g(x))
$$  
Пример такой функции:  
$$
f(x) = \sin(x^2)
$$  

### Правило дифференцирования сложной функции  
Пусть есть функция:  
$$
f(x) = h(g(x))
$$  
тогда её производная:  
$$
f'(x) = h'(g(x)) \cdot g'(x)
$$  
То есть, чтобы взять производную от сложной функции, надо сначала взять производную от внешней функции (например, от $\sin$), а потом — от внутренней ($x^2$).  

![[images/Pasted image 20250723181942.png]]  
Всё! Теперь точно переходим к ML.

## Оптимизация линейной регрессии  
Начнём издалека и вспомним обобщённую формулу [[0_0 что такое линейная регрессия|линейной регрессии]]:  
$$
M(w, x) = w_0 + \sum_{i=1}^N w \cdot x_i
$$  
А её основной лосс:  
$$
MSE = \frac{1}{N} \sum_{i=1}^{N}(y_{\text{true}} - y')^2
$$  
$y'$ — машинное предсказание.  

Так, теперь будем думать логически. Оптимизация линейной регрессии строится на том, чтобы подобрать идеальные $w_0$ и $w$, так как именно по ним строится прямая. Зачем это делается? Чтобы уменьшить лосс — значение MSE. То есть главная задача — это минимизировать лосс-функцию. Почему именно её? Потому что именно она показывает, насколько ошиблась наша модель.  

![[/images/Pasted image 20250723185330.png]]  
Построим график лосс-функции — зависимость веса от значения лосс-функции.  

![[images/Pasted image 20250723185504.png]]  
![[images/Pasted image 20250723185540.png]]  
Визуально мы можем выяснить такие $w$, при которых значение $L$ меньше всего — это значения:  
- $w_1$  
- $w_2$  
- $w_3$  

Важно понимать, что $w_1$ — это локальный минимум, то есть на каком-то ограниченном отрезке это и вправду минимум, но на всей области — нет.  

Мы, люди, существа глазастые и понимаем визуально, где минимум, а машина тупая — не понимает. В дело входит градиентный спуск!


![[images/Pasted image 20250723185931.png]]
Мы берём рандомный вес (пусть будет $w_4$) и вычисляем значение лосса.  

![[images/Pasted image 20250723190433.png]]  
Считаем градиент в данной точке ($w_4$) и получаем вектор, который показывает, куда надо идти, чтобы лосс **повышался**. Это нам не нужно — нам как раз надо пойти против этого направления, то есть в направлении **антиградиента**.  

![[images/Pasted image 20250723190645.png]]  
Отлично, теперь мы знаем, куда двигаться. Теперь нужно итерационно обновлять веса по формуле:  
$$
w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{dL}{dw_{\text{old}}}
$$  
![[images/Pasted image 20250723191132.png]]  
И так мы будем спускаться до $w_3$ (да, номера для весов выбраны странным образом, но допустим, что это просто рандомные числа, а не порядковый номер :) )  

Так, ну что, мы оптимизируем лосс-функцию, беря от неё градиент по $w$ и $w_0$ — это понятно. Но как это реализуется?

Давайте взглянем на линейную регрессию и MSE под другим углом.  

$$
MSE = \frac{1}{n} \sum_{i=1}^N(y_{\text{true}} - y')^2
$$  

Что такое $y'$? Не только предсказание — в первую очередь, это линейная регрессия. Так что если расписать MSE поподробнее, то:  

$$
MSE = \frac{1}{n} \sum_{i=1}^{N}(y_{\text{true}} - (w_0 + \sum_{i=1}^N w \cdot x_i))^2
$$  

Получается на вид страшная формула, но на самом деле мы просто заменили $y'$ на формулу линейной регрессии.  

Ок... А как оптимизировать это всё...? Тааак, ну, подумаем логически... Что перед нами? Правильно — сложная функция! Внутри функции MSE находится функция линейной регрессии (почти каждая, если не каждая модель в ML — это функция). То есть в итоге мы берём частные производные от функции $L$ (MSE) по $w$ и $w_0$.  

Итоговые формулы:  
$$
\frac{\partial \text{MSE}}{\partial w} = \frac{2}{n} \sum_{i=1}^n x_i \left( \hat{y}_i - y_i \right)
$$  
$$
\frac{\partial \text{MSE}}{\partial w_0} = \frac{2}{n} \sum_{i=1}^n \left( \hat{y}_i - y_i \right)
$$  
$$
\text{где} \quad \hat{y}_i = w x_i + w_0
$$  

Если резюмировать, то оптимизация линейной регрессии строится на основе взятия градиента от её лосс-функции по её параметрам $w$ и $w_0$, ибо мы хотим её минимизировать. В процессе градиентного спуска мы находим градиент и идём в противоположном направлении от него, используя формулу для итерационного обновления весов.
