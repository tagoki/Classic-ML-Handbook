## Недообучение (underfitting)

**Недообучение** — это ситуация, при которой модель **не способна уловить закономерности в данных**. Название говорит само за себя.

Примером недообучения может быть линейная регрессия с большой ошибкой:

![[images/Pasted image 20250729144356.png]]

На графике выше — пример недообученной линейной регрессии: линия предсказания явно не отражает зависимость в данных. В результате, ошибка — например, [[0_2_2 MSE|MSE]] — будет высокой. Такая модель **не способна делать точные предсказания на новых данных**.

---
### Причины недообучения

- ==Слишком маленький dataset== — чем меньше данных, тем хуже модель может обучиться.
- ==Неправильно подобранные параметры== — например, слишком малое количество итераций обучения.
- ==Слишком большой learning rate== при [[0_4 градиентный спуск|градиентном спуске]] — веса не успевают сойтись к минимуму.
- ==Слишком простая модель== — например, линейная регрессия не сможет аппроксимировать сложную нелинейную зависимость (например, сигмоиду).
--- 
### Методы борьбы с недообучением

- ==Увеличить dataset== — добавить новые данные или использовать методы увеличения данных.
- ==Настроить параметры обучения== — подобрать правильное количество итераций, learning rate и другие гиперпараметры.
- ==Усложнить модель== — добавить скрытые слои в нейронной сети, использовать полиномиальную регрессию или другие более гибкие методы.