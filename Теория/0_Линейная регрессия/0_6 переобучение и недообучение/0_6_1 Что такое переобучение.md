## Переобучение (overfitting)

**Переобучение** — на практике более частая проблема, при которой, в отличие от [[0_6_0 Что такое недообучение|недообучения]], модель "запоминает" данные, выучивает частные зависимости, но **не учится обобщать**.

![[images/Pasted image 20250729150653.png]]

На графике изображена переобученная модель: она буквально проходит через все обучающие точки, обеспечивая почти нулевую ошибку. На первый взгляд это кажется хорошим результатом, но реальность жёстка:  
**при работе с новыми данными такая модель ведёт себя плохо**,  
==выдавая очень маленький лосс на обучающей выборке и большой лосс на валидационной==.  
Это, кстати, и есть один из главных признаков переобучения.

---

### Причины переобучения

- ==Слишком сложная модель== — например, использование многослойной нейросети для простой задачи или применение полиномиальной регрессии при линейной зависимости.
- ==Слишком большое количество итераций== — модель слишком долго обучается и начинает запоминать конкретные примеры.
- ==Слишком много нерелевантных признаков== — это увеличивает сложность модели, что ведёт к переобучению.

---

### Методы борьбы с переобучением

- ==Упростить модель== — например, удалить ненужные признаки или уменьшить количество параметров.
- ==Сократить количество итераций обучения==
- ==Использовать регуляризацию==: [[0_4_1 L1 регуляризация (Lasso)|L1]], [[0_4_2 L2 регуляризация (Ridge)|L2]], [[0_4_3 ElasticNet|ElasticNet]]
