## L2 регуляризация
Идея схожа с [[0_4_1 L1 регуляризация (Lasso)|L1 регуляризацией]], так как тоже предназначена для регуляризации весов, а также имеет лямбда коэффициент. 

[[0_1_3_1 Формула L2 регуляции (Ridge)|Формула:]]
$$
L(w) = L + \lambda \sum_{j=1}^{n} w_j^2 
$$
Одно из главных отличий в формуле — это использование квадрата. В остальном идея расчёта такая же, как и в L1 регуляризации. 

### Плюсы L2 регуляризации
- ==Дифференцируемость==. Функция гладкая и непрерывна, так что можно использовать [[0_4 градиентный спуск|градиентный спуск]]
- ==Не зануляет признаки, а сглаживает==. Это полезно, если в наших данных есть важные признаки и не хочется их терять. 
- ==Борьба с переобучением==. Также борется с переобучением при большом количестве признаков. 
### Минусы L2 регуляризации 
- ==Не зануляет признаки==. Если вы хотите, чтобы регуляризация выполняла отбор признаков, то L1 будет получше. 
- ==Также страдает из-за== плохого масштаба 
### Пример вычисления L2 регуляризации
$$
L + \lambda \sum_{j=1}^{n} w_j^2 = 2^2 + (-3)^2 = 4 + 9 = 13
$$
L2 регуляризации является более "мягким" решением, чем L1, так как она имеет менее агрессивное поведение к весам (не зануляет их). Это отличное решение в задачах, где важна дифференцируемость и все признаки важны.     
